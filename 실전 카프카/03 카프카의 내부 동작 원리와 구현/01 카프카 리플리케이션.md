# 카프카 리플리케이션 
  
고가용성 분산 스트리밍 플랫폼인 카프카는 무수히 많은 데이터 파이프라인의 정중앙에 위치하는 **메인 허브 역할을 한다.**        

![9961CF3C5C0FBA462C](https://user-images.githubusercontent.com/50267433/150273392-d8e9d94b-2ed8-498f-a458-d348e65f1e98.png)
  
중앙에서 메인 허브 역할을 하는 카프카 클러스터가 어떠한 문제로 인해 정상적으로 동작하지 못한다면         
카프카의 연결된 전체 데이터 파이프라인에 영향을 미치게되므로 이는 매우 심각한 문제가 아닐 수 없다.     
     
따라서 카프카는 초기 설계 단계에서부터 장애가 발생하더라도   
중앙 데이터 허브로서 언정적인 서비스가 운영될 수 있도록 설계가 되어있다.      
  
대표적인 기술 중 하나로 안정성을 확보하기 위해 카프카 내부에서는 리플리케이션이 기능을 지원한다.     

# 리플리케이션 동작 개요 
  
카프카는 **브로커의 장애에도 불구하고 연속적으로 안정적인 서비스를 제공함으로써 데이터 유실을 방지하여 유연성을 제공한다.**          
카프카의 리플리케이션 동작을 위해 토픽 생성시 **필숫값** 으로, `replication factor` 옵션을 설정하도록 되어있다.      
  
## 간단 실습 

설치한 브로커 서버 중 한대로 접근을 해서 리플리케이션 토픽을 하나 생성해보려 한다.(peter-kafka01)   

* 토픽 이름 : peter-test01
* 파티션 수 : 1
* 리플리케이션 팩터 수 : 3

```console
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 \
--create \
--topic peter-test01 \
--partitions 1 \
--replication-factor 3

Created topic peter-test01
```
위와 같이 명령어 실행 후, 생성된 토픽의 정보를 알기 위해서 `describe` 명령어를 사용할 수 있다.   

```console
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 \
--topic peter-test01 \
--describe

Topic: peter-test01 PartitionCount: 1 ReplicationFactor: 3 Configs: segment.bytes=1073741824 
Topic: peter-test01 Partition: 0 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3
```
* 1줄 : 
    * 토픽의 파티션 수인1과 리플리케이션 펙터(토픽) 수인 3이 표시되어있다.      
* 2줄 : 
    * 파티션0에 대한 내용이다.   
    * 파티션0의 리더는 브로커1이다.(리플리케이션중 1번)  
    * 리플리케이션들은 현재 3개가 있음을 나타낸다.(현재 동기화하고 있는 리플리케이션들은 브로커 1,2,3이라는 의미)      
    * 여기서 중요한 사실은 실제로 리플리케이션되는 것은 토픽이 아니라 토픽을 구성하는 파티션들이다.  

```console
/usr/local/kafka/bin/kafka-console-producer.sh --boot-server peter-kafka01.foo.bar:9092 \
--topic peter-test

> test message1
```  
`test message1`메시지를 `peter-test01` 이라는 토픽으로 전송했다.        
이후, 세그먼트 파일에 저장되어있는지 살펴보자     
    
```console
/usr/local/kafka/bin/kafka-dump-log.sh \  
--print-data-log--files /data/kafka-logs/peter-test01-0/00000000000000000000.log
```
```console
Dumping /data/kafka-logs/peter-test01-0/00000000000000000000.log  
Starting offset: 0
baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 produerId: -1 
producerEpch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false
position: 0 CreateTime: 161008070323 size 81 magic: 2 compresscodec: NONE   
crc: 3417270022 isvalid: true
| offset: 0 CreateTime: 161008070323 ketysize: -1 valuesize: 13 sequence: -1   
headerKeys[] payload: test message1
``` 
* 시작 오픗세은 0임을 알 수 있다.       
* 메시지 카운트는 1임을 알 수 있다.      
* 프로듀서를 통해 보낸 메시지는 test message1 임을 알 수 있다.   

현재 서버는 peter-kafka01 이지만, 카프카 클러스터를 이루는 다른 브로커들에서도 dump 명령어를 실행하면 결과는 같다.   
즉, 콘솔 프로듀서로 보낸 메시지 하나를 총 3대의 브로커들이 모두 갖고 있는 것이다.(덤프 명령어로 접근 가능)     
        
이렇게 카프카는 `replication factor` 옵션을 이용해 관리자가 지정한 수만큼의 리플리케이션을 가질수 있다.      
**N개의 리플리케이션의 경우 `N-1` 까지의 브로커 장애가 발생해도 메시지 손실없이 안정적으로 메시지를 주고받을 수 있다.**         
    
예시 토픽인, `peter-test01`을 기준으로 설명하자면, 총 3개의 리플리케이션들의 요청을 안전하게 처리할 수 있다.     
  
# 리더와 팔로워   

```console
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 \
--topic peter-test01 \
--describe

Topic: peter-test01 PartitionCount: 1 ReplicationFactor: 3 Configs: segment.bytes=1073741824 
Topic: peter-test01 Partition: 0 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3
```
 
토픽 상세보기 명령어를 실행해보면, 출력 내용 중 파티션의 리더라는 부분이 있다.        
**모두 동일한 리플리케이션이라 하더라도 리더만의 역할이 따로 있기 때문에 카프카에서는 리더를 특별히 강조한다.**    
카프카는 내부적으로 동일한 리플리케이션들을 **리더**와 **팔로워**로 구분하고, 각자 역할을 분담시킨다.       

[#](#) 

* **리더 :** 
    * 리플리케이션 중 하나가 선정되며, **모든 읽기와 쓰기는 리더를 통해서만 가능하다.**             
    * 프로듀서는 모든 리플리케이션에 메시지를 보내는 것이 아니라 리더에게만 메시지를 전송한다.       
    * 컨슈머도 오직 리더로부터 메시지를 가져온다.   
* **팔로우 :**
    * 팔로워들 역시 리더에 문제가 발생하거나 이슈가 있을 경우를 대비해 언제든지 새로운 리더가 될 준비를 한다.    
    * 지속적으로 파티션의 리더가 새로운 메시지를 받았는지 확인하고, 새로운 메시지가 있다면 해당 메시지를 리더로부터 복제한다.   
   
# 복제 유지와 커밋   
  
리더와 팔로워는 **`ISR`이라는 논리적 그룹으로 묶여있다.**          
**ISR 그룹에 안에 속한 팔로워들만이 새로운 리더의 자격을 가질 수 있기 때문이다.**           
다시 말해, ISR 그룹에 속하지 못한 팔로워는 새로운 리더의 자격을 가질 수 없다.          
    
**ISR 그룹내의 리더와 팔로워**  
* 리더 : ISR 내 모든 팔로워가 메시지를 받을 때까지 기다린다(성능상 이슈 포인트)   
* 팔로워 : ISR 내 리더와의 데이터 일치를 유지하기 위해 지속적으로 리더의 데이터를 따른다.     

## ISR 그룹핑 이유 

팔로워가 네트워크 오류, 브로커 장애 등 여러 이유로 리더로부터 리플리케이션하지 못하는 경우도 발생할 수 있다.           
팔로워는 이미 리더와의 데이터가 불일치한 상태에 놓이게 되고, 만약 새로운 리더로 임명된다면?            
데이터의 정함섭이나 메시지 손실등의 문제가 발생할 수 있다.         
             
따라서, 파티션의 리더는 팔로워들이 뒤쳐지지 않고 리플리케이션 동작을 잘하고 있는지를 감시해야한다.               
리더에 뒤처지지 않는 팔로워들만이 ISR 그룹에 속하게 되고, 장애가 발생할 경우 리더 자격을 얻을 수 있던 것이다.         

## 동작 판단 기준 
**리더와 팔로워 중 리프리케이션 동작을 잘하고 있는지 여부 등은 누가 어떤 기준으로 판단할까? 🤔**       
  
* **리더**는 읽고 쓰는 동작은 물론, 팔로워가 리플리케이션 동작을 잘 수행하고 있는지도 판단한다.    
        
만약 팔로워가 특정 주기의 시간만큼 복제 요청을 하지 않는다면        
리더는 해당 팔로워가 리플리케이션 동작에 문제가 발생했다고 판단해 ISR 그룹내에서 추방한다.        
즉, **해당 팔로워는 새로운 리더가 될 자격을 박탈당하게 되는 것이다.**      
    
카프카 클러스터 운영중 특정 토픽의 상태가 의심되거나 문제가 있다고 판단되며     
토픽 상세보기 명령어를 통해 현재 ISR 상태를 점검해봄으로써,         
현재 토픽의 상태가 양호한지 불량한지 등을 육안으로 확인할 수 있다.   

ISR 내에서 모든 팔로워의 복제가 완료되면, **리더는 내부적으로 커밋되었다는 표시를 하게 됩니다.**      
마지막 커밋 오프셋 위치는 **하이워터마크**라고 부른다.         
즉, 커밋되었다는 것은 `replication factor` 수의 모든 리플리케이션이 전부 메시지를 저장했음을 의미한다.     
그리고 이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있다.           
카프카에서 커밋되지 않은 메시지를 컨슈머가 읽을 수 없게 하는 이유는 바로 메시지의 일관성을 유지하기 위해서다.      

[#](#)  
   
`peter-test01` 토픽을 표현한 그림으로,         
해당 토픽은 1개의 파티션과 `3개의 replication factor`로 설정됐다.         
리플리케이션 동작을 설명하기 위해 0번 파티션의 리더와 팔로워를 표시했다. 


