# 카프카의 확장  

초기 카프카 환경을 구성할 때, 트래픽과 메시지량을 고려하여 규모를 산정하지만 이를 정확히 예측하기는 어렵다.   
그러므로 트래픽과 사용량이 증가된다면 이에따라 확장도 고려해야한다.   
카프카는 이렇게 폭발적으로 사용량이 증가하는 경우를 고려해 안전하고 손쉽게 확장할 수 있도록 디자인되었다.   

이번에는 카프카의 사용량이 폭발적으로 증가해서 카프카를 확장하는 경우를 실습해보자.   

## 테스트 토픽 생성 
```shell
/usr/local/kafka/bin/kafka-topics.sh    
--bootstrap-server peter-kafka01.foo.bar:9092 
--create 
--topic peter-scaleout1
--partitions 4 
--replication-factor 1

> Created topic peter-scaleout1
```
생성된 토픽을 파악하기 위해서 토픽 상세보기를 진행하자 

```shell
/usr/local/kafka/bin/kafka-topics.sh     
--bootstrap-server peter-kafka01.foo.bar:9092
--describe
--topic peter-scaleout1
``` 

[#](#)  

출력된 내용을 그림으로 표현하면 위와 같다.   
카프카의 확장 실습을 위해 브로커를 하나 더 추가할 것이고 주키퍼 서버중 하나인 zk03을 이용하려고한다.     
책에서 알려주는 명령어를 입력해서 카프카를 설치했다면 설정 파일을 편집하자.  

```shell
sudo vi /usr/local/kafka/config/server.properties 
```
`/usr/local/kafka/config/server.properties` 설정 파일에서   
`broker.id=4`가 잘 지정되었는지 확인한다.     
`broker.id=4`로 설정하는 이유는 현재 브로커들이 1,2,3 으로 설정되어있기 때문이다.(고유해야함)   

이제 다음과 같이 systemctl 명령어를 이용해 카프카가 실행중인 상태인지 확인한다.(running 확인)     
```shell 
sudo systemctl status kafka-server 
``` 
 
브로커를 추가한 다음, 기존의 토픽과 파티션은 어떻게 될까?         
카프카는 확장을 위해 브로커를 한대 추가한 상태이다.       
**카프카의 부하가 높은 상황이라 긴급하게 브로커를 한대 추가했지만, 기존의 토픽과 파티션들은 변화가 없다.**     
즉, **브로커 추가 작업은 간단히 끝나지만 토픽과 파티션들은 새로 추가된 브로커로 이동하지 않는다.**      
**관리자가 수작업으로 토픽의 파티션들을 고루 분산시켜줘야한다.**     
         
브로커를 추가한 후 토픽을 생성했을 때 브로커별로 잘 분산되는지 확인해보자.   

```shell
/usr/local/kafka/bin/kafka-topic.sh   
--bootstrap-server peter-kafka01.foo.bar:9092.   
--create   
--topic peter-caleout2  
--partitions 4
--replication-factor 1

> Created topic peter-scaleout2 
```    

토픽 생성이 완료되면 다음과 같이 토픽 상세보기를 통해 파티션들이 어느 브로커에 위치하고 있는지를 확인한다.   

```bash
/usr/local/kafka/bin/kafka-topics.sh   
--bootstra-server peter-kafka01.foo.bar:9092   
--describe 
--topic peter-scaleout2

> 토픽에 대한 정보가 나온다.   
```  

토픽은 생성했을때의 설정값 그대로    
파티션이 4개로 새롭게 확장한 브로커 4번을 포함해 브로커 ID당 하나의씩 고르게 분포되어있음을 알 수 있다.  
브로커 4번을 추가하기 전에, 생성한 토픽 peter-scaleout1과 peter-scaleout2 토픽을 살펴보자.    

[#](#)    
  
위 그림과 같이  
브로커 4번을 추가하기 전에 생성한 토픽 peter-scaleout1 은 그대로 유지되고 있다.     
브로커 4번을 추가한 후에 생성한 토픽 peter-scaleout2 는 고르게 유지되고 있다.    

이러한 현상은 브로커의 리소스 측면에서 다소 비효율적일 수 있다.   
   
카프카 클러스터의 부하가 전체적으로 높은 상태이며  
특히 peter-kafka03 브로커의 부하가 가장 높은 상황이라 가정하자   
관리자가 브로커들의 부하를 분산시키기 위한 목적으로 브로커를 추가해도 기존 파티션은 그대로 남아있다.  
즉, 부하는 계속 높은 상황이 이어질 것이며 문제가 해결되지는 않을 것이다.   

## 브로커의 부하 분산 
 
전체 브로커들에게 토픽의 파티션을 고르게 부하 분산하기 위해서는      
새로 추가된 브로커를 비롯해 모든 브로커에게 균등하게 파티션을 분산시켜야한다.    
카프카에서 제공하는 `kafka-reassign-partitions.sh`라는 도구를 이용하면 파티션을 이동시킬 수 있다.    
이 도구를 이용해 실습용 토픽의 파티션을 분산시키자.   
     
우선, 분산 작업이 어떤 순서로 이루어지는지를 살펴보자.  
1. **분산시킬 대상 토픽을 정한다.
2. **토픽의 파티션들을 어느 브로커로 분산시킬지를 정한다.**    
3. 파티션 이동 작업을 위해 정해진 JSON 포맷으로 파일을 생성한다.  
 
**resassign-partitions-topic.json**.  
```json
{
    "topics" : 
        [{"topic" : "peter-scaleout1"}, {"topic" : "peter-scaleout2"}],
        "version" : 1
}
```
peter-scaleout1, peter-scaleout2 토픽을 추가한 JSON 파일이다.       
작업 대상을 `topics` 안에 넣으면 된다.  

```shell 
/usr/local/kafka/bin/kafka-reassign-partitions.sh
--bootstrap-server peter-kafka01.foo.bar:9092
--generate
--topics-to-move-json-file reassign-partitions-topic.json. 
--broker-list "1,2,3,4"

> 실행결과 
```  
분산시킬 대상 토픽에 대한 JSON 파일이 생성되었다면,   
`kafka-reassign-partitions.sh` 명령어를 이용해 파티션을 분산시킬 브로커 리스트를 지정한다.      
여기서는 1,2,3,4 브로커 모두를 지정했다.   

실행한 결과를 살펴보면,   
peter-scalueout1 토픽의 현재 설정된 파티션 배치를 가장 먼저 보여주고 이후 제안하는 파티션 배치가 출력되었다.    
이제 제안된 파티션 배치 설정을 복사한 후 새로운 move.js 파일을 생성한다.  

```json 
~~~
```   
  
kafka-reassign-partitions.sh 도구를 이용해     
peter-scaleout1 토픽의 제안된 파티션 배치의 내용을 복사한 예제이다.      
위 내용과 동일하게, move.json 파일을 생성했다면 이제 peter-scaleout1 토픽에 대해서 파티션 배치를 실행하자.    
이제 kafka-reassign-partitions.sh 명령어와 --reassignment-json-file 옵션으로     
move.json 을 정의해 peter-scaleout 토픽에 대해 파티션 배치를 실행하자.   

```shell
/usr/local/kafka/bin/kafka-ressign-partitions.sh
--bootstrap-server peter-kafka01.foo.bar:9092   
--reassignment-json-file- move.json
--execute

> 실행결과 나옴 
```   
출력 내용을 보면 재배치가 성공적으로 실행됬을을 알 수 있다.  
고르게 배치되었는지 describe로 확인해보자.   
    
```shell
/usr/local/kafka/bin/kafka-ressign-partitions.sh
--bootstrap-server peter-kafka01.foo.bar:9092   
--describe
--topic peter-sacleout1  

> 실행결과 나옴
``` 

[#](#)  
  
실행결과를 통해 고르게 배치된 것을 확인할 수 있다.     
관리자 입장에서는 카프카 클러스터에 브러커를 추가하면 카프카의 로드가 자동 분산되리라 기대했겠지만,    
실제로는 기존 브로커들이 갖고 있던 파티션들은 자동으로 분산되지 않는다.    
하지만, 신규 브로커가 추가된 이후에 생성하는 토픽들은 신규 브로커를 비롯해 파티션들을 분산 배치한다.   
 
브로커 간의 부하 분산 밸런스를 맞추려면     
관리자는 기존 파티션들이 모든 브로커에 고르게 분산되도록 수동으로 분산 작업을 진행해야햔다.    
실제 운영을 하다보면 균형이 맞지 않을때도 종종 발생한다.     
이런 경우에 파티션을 재배치한다면, 좀 더 효율적으로 브로커의 리소스를 사용할 수 있을 것이다.  

## 분산 배치 작업시 주의사항 

분산배치 작업을 수행할 때는 버전 업그레이드 작업과 마찬가지로 카프카의 사용량이 낮은 시간에 진행하는 것을 추천한다.   
카프카에서 파티션이 재배치되는 과정은 파티션이 단순하게 이동하는 것이 아니라     
**브로커 내부적으로 리플리케이션 동작이 일어나기 때문이다.**   

[#](#)  

peter-kafka01에 위치하는 peter-scaleout의 3번 파티션이 peter-kafka02로 이동하는 과정을 보여준다.   
 
재배치 명령어가 실행되면       
1. 이동 대상 파티션을 목적지 브로커에 리플리케이션하게 된다.     
2. 리플리케이션이 완료된 후, 이동하기 전 브로커에 위치한 파티션은 삭제된다.     
3. 결과적으로 마치 이동한 것처럼 보이지만, 내부적으로는 리플리케이션 동작이 일어나는 것이다.  
  
설명을 단순하게 파티션 하나만 이동하는 것으로 가정했지만    
토픽의 파티션이 10개이고 리플리케이션 팩터가 3개이면, 총 30개 파티션들의 리플리케이션 동작이 일어나게 된다.     
그리고 이러한 동작은 브로커에게 과중한 부하를 주게된다.     
    
브로커에게 부하를 주는 것 뿐만아니라    
리플리케이션으로 인한 네트워크 트래픽 사용량도 급증하여 서버 관리자나 네트워크 관리자는 몹시 당황하게 될 것이다.       
물론 재배치하고자 하는 토픽의 용량이 작고 사용량이 높지 않으면, 큰 문제는 발생하지 않지만     
안전하게 작업을 하고자 한다면 되도록 브로커의 사용량이 낮은 시간을 이용해 파티션 재배치 작업을 수행해야한다.    
   
추가로 한가지 팁이 더 있다.      
용량이 큰 토픽의 파티션을 반드시 재배치하는 상황이라 가정하자.    
좀더 구체적으로 파티션 하나의 크기가 약 700G 라고 가정하고 해당 토픽의 보관 주기는 기본값인 1주일이라고 가정한다.  
이때 파티션 하나당 700G 크기의 파티션을 아무런 수정없이 다른 브로커로 재배치할 수도 있다.  
하지만 만약 해당 토픽의 메시지들은 모두 컨슈머가 최근의 내용까지 모두 컨슘했고, 앞으로 재처리할 일이 없다면   
최근 메시지를 제외한 나머지 메시지들은 모두 삭제해도 무방할 것이다.   
 
따라서 임시로 해당 토픽의 보관 주기를 1주일에서 1일로 변경한다면 700 -> 100G로 줄어들 것이고     
파티션의 크기를 줄이고 난 후 재배치 작업을 진행핞다면 기존 대비 브로커의 부하나 네트워크 사용량도 줄일 수 있다.     
즉, 불필요한 메시지들을 재배치하면서 굳이 더 많은 리소스를 소모할 필요는 없다.    

또 하나의 팁은 파티션 재배치 작업시 여러 개의 토픽을 동시에 진행하지 않고, 단 하나의 토픽만 진행하는 것이다.   
파티션 재배치 작업은 결국 카프카 내부적으로 리플리케이션 동작이 일어나는 것이고,       
여러 개의 토픽을 동시다발적으로 진행한다면 그만큼의 부하가 브로커에게 그대로 전달된다.    
따라서 다소 작업 시간이 소요되더라도 최대한 카프카의 안전성을 목표로 한다면, 한번에 하나의 토픽만 진행하자.  


 





'




